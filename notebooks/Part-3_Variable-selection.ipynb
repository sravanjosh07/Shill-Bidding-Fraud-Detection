{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b62177",
   "metadata": {},
   "source": [
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7132603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 03:38:22.523682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize, MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from hpelm import ELM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc799208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Group_14_data_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27482eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = data.drop(['Class', 'Bidder_ID'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eda47fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using Lasso: Index(['Successive_Outbidding', 'Last_Bidding', 'Winning_Ratio',\n",
      "       'Auction_Duration'],\n",
      "      dtype='object')\n",
      "Selected features using correlations:  ['Successive_Outbidding', 'Bidding_Ratio', 'Winning_Ratio', 'Bidder_Tendency']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Standardize the features using the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit a Lasso regression model on the training data\n",
    "alpha = 0.004  # regularization strength\n",
    "lasso = Lasso(alpha=alpha)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Select the features with non-zero coefficients\n",
    "selected_features_Lasso = X.columns[lasso.coef_ != 0]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features using Lasso:\", selected_features_Lasso)\n",
    "\n",
    "\n",
    "corr = data.corr()['Class'].sort_values(ascending=False)\n",
    "selected_features = corr[corr > 0.1].index.tolist()\n",
    "selected_features.remove('Class')\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features using correlations: \", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332e4d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class                     1.000000\n",
      "Successive_Outbidding     0.901035\n",
      "Bidding_Ratio             0.569435\n",
      "Winning_Ratio             0.394122\n",
      "Bidder_Tendency           0.295533\n",
      "Last_Bidding              0.097655\n",
      "Early_Bidding             0.053570\n",
      "Auction_Bids              0.044964\n",
      "Starting_Price_Average    0.042604\n",
      "Auction_Duration          0.021145\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f0853",
   "metadata": {},
   "source": [
    "let's look at correlations. Correlation measures how strong a relationship exists between two variables. In feature selection, we look at the correlation between the independent variables and the target variable. If the correlation is high, it indicates that the independent variable has a strong relationship with the target variable and hence, may be a good predictor. In this case, we can see that 'Successive_Outbidding', 'Starting_Price_Average', 'Early_Bidding', 'Winning_Ratio', and 'Class' are highly correlated with each other.\n",
    "\n",
    "However, correlation alone may not be the best method for feature selection, as it only measures linear relationships between variables. It may not capture the complex relationships that may exist in the data. In other words, two variables may be highly related, but not necessarily in a linear fashion.\n",
    "\n",
    "This is where Lasso comes in. Lasso is a regularization technique that is commonly used for feature selection. It works by shrinking the coefficients of less important variables towards zero, effectively reducing the number of features in the model. Lasso is particularly useful when dealing with high-dimensional data, where the number of features is much larger than the number of samples.\n",
    "\n",
    "In this case, we can see that Lasso has selected a subset of features that are important for predicting the target variable, namely 'Bidder_Tendency', 'Successive_Outbidding', 'Last_Bidding', 'Starting_Price_Average', 'Winning_Ratio', and 'Auction_Duration'. By using Lasso, we can effectively reduce the number of features while still maintaining good predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52913e44",
   "metadata": {},
   "source": [
    "# Training the best models from part 2 and part 3  on selected features using **LASSO**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35b4c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = data[['Bidder_Tendency', 'Successive_Outbidding', 'Last_Bidding',\n",
    "       'Winning_Ratio']]\n",
    "# X = data[['Successive_Outbidding', 'Bidding_Ratio', 'Winning_Ratio', 'Bidder_Tendency']]\n",
    "y = data['Class']\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27d5600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "def create_model_(num_layers=1, num_neurons=10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=X_train_selected.shape[1], activation='relu'))\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(num_neurons, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f86b5945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "158/158 - 1s - loss: 0.2040 - accuracy: 0.9337 - 622ms/epoch - 4ms/step\n",
      "Epoch 2/50\n",
      "158/158 - 0s - loss: 0.0442 - accuracy: 0.9802 - 199ms/epoch - 1ms/step\n",
      "Epoch 3/50\n",
      "158/158 - 0s - loss: 0.0435 - accuracy: 0.9812 - 205ms/epoch - 1ms/step\n",
      "Epoch 4/50\n",
      "158/158 - 0s - loss: 0.0412 - accuracy: 0.9818 - 219ms/epoch - 1ms/step\n",
      "Epoch 5/50\n",
      "158/158 - 0s - loss: 0.0415 - accuracy: 0.9806 - 200ms/epoch - 1ms/step\n",
      "Epoch 6/50\n",
      "158/158 - 0s - loss: 0.0400 - accuracy: 0.9814 - 207ms/epoch - 1ms/step\n",
      "Epoch 7/50\n",
      "158/158 - 0s - loss: 0.0418 - accuracy: 0.9804 - 205ms/epoch - 1ms/step\n",
      "Epoch 8/50\n",
      "158/158 - 0s - loss: 0.0405 - accuracy: 0.9800 - 198ms/epoch - 1ms/step\n",
      "Epoch 9/50\n",
      "158/158 - 0s - loss: 0.0392 - accuracy: 0.9832 - 200ms/epoch - 1ms/step\n",
      "Epoch 10/50\n",
      "158/158 - 0s - loss: 0.0393 - accuracy: 0.9818 - 201ms/epoch - 1ms/step\n",
      "Epoch 11/50\n",
      "158/158 - 0s - loss: 0.0390 - accuracy: 0.9822 - 202ms/epoch - 1ms/step\n",
      "Epoch 12/50\n",
      "158/158 - 0s - loss: 0.0392 - accuracy: 0.9804 - 201ms/epoch - 1ms/step\n",
      "Epoch 13/50\n",
      "158/158 - 0s - loss: 0.0391 - accuracy: 0.9822 - 203ms/epoch - 1ms/step\n",
      "Epoch 14/50\n",
      "158/158 - 0s - loss: 0.0402 - accuracy: 0.9812 - 201ms/epoch - 1ms/step\n",
      "Epoch 15/50\n",
      "158/158 - 0s - loss: 0.0376 - accuracy: 0.9826 - 191ms/epoch - 1ms/step\n",
      "Epoch 16/50\n",
      "158/158 - 0s - loss: 0.0384 - accuracy: 0.9820 - 192ms/epoch - 1ms/step\n",
      "Epoch 17/50\n",
      "158/158 - 0s - loss: 0.0386 - accuracy: 0.9826 - 196ms/epoch - 1ms/step\n",
      "Epoch 18/50\n",
      "158/158 - 0s - loss: 0.0385 - accuracy: 0.9810 - 196ms/epoch - 1ms/step\n",
      "Epoch 19/50\n",
      "158/158 - 0s - loss: 0.0402 - accuracy: 0.9814 - 193ms/epoch - 1ms/step\n",
      "Epoch 20/50\n",
      "158/158 - 0s - loss: 0.0386 - accuracy: 0.9814 - 191ms/epoch - 1ms/step\n",
      "Epoch 21/50\n",
      "158/158 - 0s - loss: 0.0384 - accuracy: 0.9816 - 202ms/epoch - 1ms/step\n",
      "Epoch 22/50\n",
      "158/158 - 0s - loss: 0.0377 - accuracy: 0.9826 - 197ms/epoch - 1ms/step\n",
      "Epoch 23/50\n",
      "158/158 - 0s - loss: 0.0373 - accuracy: 0.9826 - 196ms/epoch - 1ms/step\n",
      "Epoch 24/50\n",
      "158/158 - 0s - loss: 0.0386 - accuracy: 0.9814 - 193ms/epoch - 1ms/step\n",
      "Epoch 25/50\n",
      "158/158 - 0s - loss: 0.0371 - accuracy: 0.9826 - 196ms/epoch - 1ms/step\n",
      "Epoch 26/50\n",
      "158/158 - 0s - loss: 0.0382 - accuracy: 0.9816 - 194ms/epoch - 1ms/step\n",
      "Epoch 27/50\n",
      "158/158 - 0s - loss: 0.0374 - accuracy: 0.9820 - 205ms/epoch - 1ms/step\n",
      "Epoch 28/50\n",
      "158/158 - 0s - loss: 0.0376 - accuracy: 0.9820 - 199ms/epoch - 1ms/step\n",
      "Epoch 29/50\n",
      "158/158 - 0s - loss: 0.0370 - accuracy: 0.9826 - 196ms/epoch - 1ms/step\n",
      "Epoch 30/50\n",
      "158/158 - 0s - loss: 0.0366 - accuracy: 0.9820 - 196ms/epoch - 1ms/step\n",
      "Epoch 31/50\n",
      "158/158 - 0s - loss: 0.0372 - accuracy: 0.9810 - 210ms/epoch - 1ms/step\n",
      "Epoch 32/50\n",
      "158/158 - 0s - loss: 0.0370 - accuracy: 0.9832 - 211ms/epoch - 1ms/step\n",
      "Epoch 33/50\n",
      "158/158 - 0s - loss: 0.0376 - accuracy: 0.9818 - 198ms/epoch - 1ms/step\n",
      "Epoch 34/50\n",
      "158/158 - 0s - loss: 0.0373 - accuracy: 0.9808 - 197ms/epoch - 1ms/step\n",
      "Epoch 35/50\n",
      "158/158 - 0s - loss: 0.0369 - accuracy: 0.9822 - 198ms/epoch - 1ms/step\n",
      "Epoch 36/50\n",
      "158/158 - 0s - loss: 0.0367 - accuracy: 0.9838 - 195ms/epoch - 1ms/step\n",
      "Epoch 37/50\n",
      "158/158 - 0s - loss: 0.0369 - accuracy: 0.9832 - 192ms/epoch - 1ms/step\n",
      "Epoch 38/50\n",
      "158/158 - 0s - loss: 0.0362 - accuracy: 0.9826 - 196ms/epoch - 1ms/step\n",
      "Epoch 39/50\n",
      "158/158 - 0s - loss: 0.0367 - accuracy: 0.9820 - 197ms/epoch - 1ms/step\n",
      "Epoch 40/50\n",
      "158/158 - 0s - loss: 0.0366 - accuracy: 0.9832 - 194ms/epoch - 1ms/step\n",
      "Epoch 41/50\n",
      "158/158 - 0s - loss: 0.0361 - accuracy: 0.9830 - 195ms/epoch - 1ms/step\n",
      "Epoch 42/50\n",
      "158/158 - 0s - loss: 0.0377 - accuracy: 0.9820 - 195ms/epoch - 1ms/step\n",
      "Epoch 43/50\n",
      "158/158 - 0s - loss: 0.0367 - accuracy: 0.9820 - 200ms/epoch - 1ms/step\n",
      "Epoch 44/50\n",
      "158/158 - 0s - loss: 0.0365 - accuracy: 0.9824 - 248ms/epoch - 2ms/step\n",
      "Epoch 45/50\n",
      "158/158 - 0s - loss: 0.0364 - accuracy: 0.9828 - 200ms/epoch - 1ms/step\n",
      "Epoch 46/50\n",
      "158/158 - 0s - loss: 0.0363 - accuracy: 0.9828 - 197ms/epoch - 1ms/step\n",
      "Epoch 47/50\n",
      "158/158 - 0s - loss: 0.0362 - accuracy: 0.9824 - 197ms/epoch - 1ms/step\n",
      "Epoch 48/50\n",
      "158/158 - 0s - loss: 0.0356 - accuracy: 0.9830 - 200ms/epoch - 1ms/step\n",
      "Epoch 49/50\n",
      "158/158 - 0s - loss: 0.0360 - accuracy: 0.9818 - 201ms/epoch - 1ms/step\n",
      "Epoch 50/50\n",
      "158/158 - 0s - loss: 0.0364 - accuracy: 0.9824 - 195ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0d85a6e80>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model_(num_layers=2, num_neurons=64)\n",
    "model.fit(X_train_selected, y_train_selected, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "314059d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205533596837945\n"
     ]
    }
   ],
   "source": [
    "# BEST MODEL FROM PART - 2 (Random Forest)\n",
    "# Best hyperparameters:  \n",
    "# {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
    "\n",
    "# Define the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_leaf=1, min_samples_split=5)\n",
    "rf_model.fit(X_train_selected, y_train_selected)\n",
    "y_pred_rf = rf_model.predict(X_test_selected)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f98f5f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9810\n",
      "Accuracy of the best part 2 model (Rf) on selected features : 0.7968\n",
      "Accuracy of the best part 3 model (NN) on selected features: 0.9810\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "loss, accuracy_nn = model.evaluate(X_test_selected, y_test_selected)\n",
    "# Print the accuracy without rounding\n",
    "print(\"Accuracy of the best part 2 model (Rf) on selected features : {:.4f}\".format(acc_rf))\n",
    "print(\"Accuracy of the best part 3 model (NN) on selected features: {:.4f}\".format(accuracy_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229e6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9408cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ba2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
